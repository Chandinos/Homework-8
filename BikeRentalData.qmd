---
title: "Homework8"
format: html
editor: visual
---

```{r}
library(tidyverse)
library(janitor)
library(lubridate)
library(tidymodels)
```

## Reading in Data

```{r}
#Fixing nchar error
data <- read_csv("SeoulBikeData.csv", locale = locale(encoding = "latin1"))
```

## EDA

### Checking for Missing Data

To begin the Exploratory Data Analysis, I first checked for any missing columns. Missing values can potentially introduce bias (or errors) in any modeling done down the road.

```{r}
colSums(is.na(data))
```

The results show that there is no missing data in the current data set, meaning the data set is complete.

### Checking Columns Types and Values within Columns

The next step is to check if each variable is stored under the correct data type and that the values of each variable make sense given the variable definitions.Categorical variables need to have their unique values checked as well.

```{r}
glimpse(data)
summary(data)
lapply(data[c("Seasons", "Holiday", "Functioning Day")], unique)
```

The data set appears to be clean. Categorical variables have expected values; for example, **Seasons** has values such as "Winter", **Holiday** has "No Holiday", and **Functioning Day** has "Yes". The numeric variables have expected ranges such as **Hours** not exceeding 24 and **Humidity(%)** not exceeding 100.

### Converting Date Column

The date column is currently read simply as text so it needs to be converted to an actual Date object.

```{r}
data$Date <- dmy(data$Date)
```

This change will allow for time-based grouping and plotting later.

### Convert Character Variables to Factors

Categorical variables are being changed from character to factor variables.

```{r}
data <- data |>
  mutate(
    Seasons = as.factor(Seasons),
    Holiday = as.factor(Holiday),
    `Functioning Day` = as.factor(`Functioning Day`)
  )
```

They have been changed to factor variables for better summaries and modeling later on.

### Rename Columns

The variables names are being converted to PascalCase making them easier to use.

```{r}
data <- data |> clean_names(case = "big_camel")
glimpse(data)
```

Now variables like **'Rented Bike Count'** appear as **RentedBikeCount** and **'Functioning Day'** appear as **FunctioningDay**.

### Create Summary Statistics

I explored RentedBikeCount across key categorical variables.

```{r}
#Five-Number Summary
summary(data$RentedBikeCount)

#RentedBikeCount Mean by Season
data |> 
  group_by(Seasons) |>
  summarise(MeanBikes = mean(RentedBikeCount))

#RentedBikeCount Mean by Holiday
data |> 
  group_by(Holiday) |>
  summarise(MeanBikes = mean(RentedBikeCount))

#RentedBikeCount Mean by Functioning Day
data |> 
  group_by(FunctioningDay) |>
  summarise(MeanBikes = mean(RentedBikeCount))

#Subsetting Data to Only Include Functioning Day = Yes
data <- data |> filter(FunctioningDay == "Yes")

```

The mean rented bike count across FunctioningDay showed that when that variable takes on a value of "no", the operation was in fact closed. Therefore, I subsetted the data to only include days in which the company was operational (FunctioningDay = "Yes").

### Summarize the Data to have only One Observation per Day

The current data set records values on an hourly basis, so the data is going to be summarized in a way that each observation represents the counts and averages for the **day**. This will allows us to work with the data on a day by day basis, not hour by hour basis.

```{r}
DailyData <- data |>
  group_by(Date, Seasons, Holiday) |>
  summarise(
    TotalBikeCount = sum(RentedBikeCount, na.rm = TRUE),
    TotalRainfallMm = sum(RainfallMm, na.rm = TRUE),
    TotalSnowfallCm = sum(SnowfallCm, na.rm = TRUE),
    MeanTempC = mean(TemperatureC, na.rm = TRUE),
    MeanHumidityPercent = mean(HumidityPercent, na.rm = TRUE),
    MeanWindSpeedMs = mean(WindSpeedMS, na.rm = TRUE),
    MeanVisibility10M = mean(Visibility10M, na.rm = TRUE),
    MeanDewPointC = mean(DewPointTemperatureC, na.rm = TRUE),
    MeanSolarMjm2 = mean(SolarRadiationMjM2, na.rm = TRUE),
    .groups = "drop"
  )

#Saving the New Data 
write.csv(DailyData, "SeoulBikeDataDaily.csv")
```

The new data set that we are going to work with contains 353 daily observations (as opposed to 8465 before) representing the 353 days out the year that the rental shop was open. It now contains summarized daily weather information and total daily rentals.

### Exploring Relationships / Coorelations (using New Data Set)

I am going to check the daily summaries using the new data set. I am also going to check the correlations between numeric variables.

```{r}
#Basic Summaries
summary(DailyData)

#Correlation (Numeric Variables)
NumericVars <- select(DailyData, where(is.numeric))
round(cor(NumericVars, use = "pairwise.complete.obs"), 2)
```

Now I am going to exploring relationships using plots. Specifically, *Humidity vs Bike Rental Total*, *Trend over Over Time*, *Holiday Rentals vs Non-Holiday Rentals*, & *Temperature vs Rentals (Colored by Season)*

```{r}
# Humidity vs Bike Rental Total 
ggplot(DailyData, aes(x = MeanHumidityPercent, y = TotalBikeCount)) + geom_point(alpha = 0.5) +
  geom_smooth(method = "lm", se = FALSE, color = "cyan") + 
  labs(title = "Daily Bike Rentals vs Humidity",
       x = "Mean Daily Humidity(%)",
       y = "Total Daily Bike Rentals")
```
This plot shows that there is not a strong relationship between humidity and daily bike rentals. The correlation is 0.04 along with the scatterplot leads up to conclude that humidity alone is not a driving force of consumer bike rental behavior.

```{r}
# Trend of Bike Rentals over Time
ggplot(DailyData, aes(x = Date, y = TotalBikeCount, color = Seasons)) + geom_line(alpha = 0.8) +
  labs(title = "Trend of Bike Rentals over Time",
       x = "Date",
       y = "Total Daily Bike Rentals")
```
In the figure above, the daily trend over time shows rentals are low in the wintertime and then steadily increases through the spring into the summer. There is a small decline in late summer/early fall with a brief rebound in September/October before tapering off as winter approaches. Although there are clear fluctuations present, the overall trend is low bike rentals over the cold periods of the year and peak bike rentals occuring in the warm periods of the year.

```{r}
#Holiday vs Non-Holiday Bike Rentals
ggplot(DailyData, aes(x = Holiday, y = TotalBikeCount, fill = Holiday)) + geom_boxplot(aplha = 0.5) +
  labs(title = "Holiday vs Non-Holiday Bike Rentals",
       x = "Holiday Status",
       y = "Total Daily Bike Rentals")
```
From the boxplot above, we can see that there are noticeably less bike rentals on holidays than on days that are not considered holidays.

```{r}
#Temperature vs Rentals (Colored by Season)
ggplot(DailyData, aes(x = MeanTempC, y = TotalBikeCount, color = Seasons)) + geom_point(alpha = 0.5) +
  labs(title = "Temperature vs Bike Rentals (Colored by Season)",
       x = "Mean Daily Temperature (Celsius)",
       y = "Total Daily Bike Rentals",
       color = "Season")
```
The figure above displays a positive relationship between temperature and rentals. The higher the temperature, the more bike rentals there are. Summer has the most rentals but it appears as it gets extremely hot, the rentals decline. Overall, warmer months promote more people to rent bikes and colder months have lower demand for bike rentals.

## Split the Data

```{r}
#For reproducibility
set.seed(1234)

#Split Data 75/25 (Stratified by Seasons)
DailyDataSplit <- initial_split(DailyData, prop = 0.75, strata = Seasons)
TrainingData <- training(DailyDataSplit)
TestData <- testing(DailyDataSplit)

#Checking Overall Ratio
nrow(TrainingData) / nrow(DailyData)
nrow(TestData) / nrow(DailyData)

#10-fold CV split
CvFolds <- vfold_cv(TrainingData, v = 10, strata = Seasons)

#Checking the split balance for Training Data and Testing Data
prop.table(table(TrainingData$Seasons))
prop.table(table(TestData$Seasons))
```


## Fitting MLR Models

### Recipe1: Creating Weekday/Weekend variable (factor), Normalizing Numeric Variables, Creating Dummy Variables
```{r}
#Creating Weekday/Weekend Factor Variable // Normalizing Numeric Variables // Creating Dummy Variables
Recipe1 <- recipe(TotalBikeCount ~ ., data = TrainingData) |>
  #Extracting day of the week
  step_date(Date, features = "dow", label = TRUE) |>
  #Creating the weekday/weekend variable
  step_mutate(
    DayType = factor(if_else(Date_dow %in% c("Sat", "Sun"), "Weekend", "Weekday"))
  ) |>
  #Removing Date and intermediate variable
  step_rm(Date, Date_dow) |>
  #Removing columns that have no variation (these columns contribute nothing to the upcoming models)
  step_zv(all_predictors()) |>
  #Normalizing the predictors (mean = 0, standard deviation = 1)
  step_normalize(all_numeric_predictors()) |>
  #Create dummy variables for Seasons, Holiday, and DayType
  step_dummy(all_of(c("Seasons", "Holiday", "DayType")))

#Prepping and Baking
Recipe1_Prep <- prep(Recipe1)
TrainingProcessed <- bake(Recipe1_Prep, new_data = TrainingData)
TestingProcessed <- bake(Recipe1_Prep, new_data = TestData)

#Quick Visual Check of Variables (No Date column // Dummy variables created // Numeric variables standardized)
glimpse(TrainingProcessed)
glimpse(TestingProcessed)
```

### Recipe2: Same as Recipe 1 + interactions between seasons & holidays, seasons and temp, temp & rainfall.
```{r}
#Same Manipulations as Recipe 1
Recipe2 <- recipe(TotalBikeCount ~ ., data = TrainingData) |>
  step_date(Date, features = "dow", label = TRUE) |>
  step_mutate(
    DayType = factor(if_else(Date_dow %in% c("Sat", "Sun"), "Weekend", "Weekday"))
  ) |>
  step_rm(Date, Date_dow) |>
  step_zv(all_predictors()) |>
  step_normalize(all_numeric_predictors()) |>
  step_dummy(all_of(c("Seasons", "Holiday", "DayType"))) |>

#Creating desired interactions

  #Seasons & Holidays
  step_interact(~matches("^Seasons_(Autumn|Spring|Summer|Winter)$")
                : matches("^Holiday_(Holiday|No\\.Holiday)$"))  |>
  #Seasons & Temperature
  step_interact(~matches("^Seasons_(Autumn|Spring|Summer|Winter)$"):MeanTempC) |>
  #Temperature & Rainfall
  step_interact(~MeanTempC:TotalRainfallMm) |>
  #Eliminating all unnecessary linear combinations
  step_lincomb(all_numeric_predictors())
  
#Prepping and Baking
Recipe2_Prep <- prep(Recipe2)
TrainingProcessed2 <- bake(Recipe2_Prep, new_data = TrainingData)
TestingProcessed2 <- bake(Recipe2_Prep, new_data = TestData)

#Quick visual check of interaction columns
TrainingProcessed2 |> select(matches("_x_")) |> glimpse()
TestingProcessed2 |> select(matches("_x_")) |> glimpse()
```

### Recipe3: Same as Recipe 2 + adding in quadratic terms for each numeric predictor
```{r}
#Same as Recipe 2
Recipe3 <- recipe(TotalBikeCount ~ ., data = TrainingData) |>
  step_date(Date, features = "dow", label = TRUE) |>
  step_mutate(
    DayType = factor(if_else(Date_dow %in% c("Sat", "Sun"), "Weekend", "Weekday"))
  ) |>
  step_rm(Date, Date_dow) |>
  step_zv(all_predictors()) |>
  step_normalize(all_numeric_predictors()) |>
  step_dummy(all_of(c("Seasons", "Holiday", "DayType"))) |>
  step_interact(~matches("^Seasons_(Autumn|Spring|Summer|Winter)$")
                : matches("^Holiday_(Holiday|No\\.Holiday)$"))  |>
  step_interact(~matches("^Seasons_(Autumn|Spring|Summer|Winter)$"):MeanTempC) |>
  step_interact(~MeanTempC:TotalRainfallMm) |>
  #Quadratic Terms for each numeric predictor
  step_poly(
    all_of(c(
      "TotalRainfallMm","TotalSnowfallCm","MeanTempC","MeanHumidityPercent",
      "MeanWindSpeedMs","MeanVisibility10M","MeanDewPointC","MeanSolarMjm2"
    )),
    degree = 2,
    options = list(raw = TRUE),
    keep_original_cols = TRUE
  ) |>
    step_lincomb(all_numeric_predictors())
  
#Prepping and Baking
Recipe3_Prep <- prep(Recipe3)
TrainingProcessed3 <- bake(Recipe3_Prep, new_data = TrainingData)
TestingProcessed3 <- bake(Recipe3_Prep, new_data = TestData)

#Quick visual check for quadratic terms
glimpse(TrainingProcessed3)
glimpse(TestingProcessed3)
```

## Model Fit
```{r}
#Ensuring reproducibility
set.seed(1234)

#Defining what model to fit and what engine to use
ModelSpec <- linear_reg() |> set_engine("lm")

#Recipe 1 workflow and cross-validation
Wf1 <- workflow() |> add_model(ModelSpec) |> add_recipe(Recipe1)

Results1 <- fit_resamples(
  Wf1, 
  resamples = CvFolds, 
  metrics = metric_set(rmse, rsq), 
  control = control_resamples(save_pred = TRUE))
  
#Recipe 2 workflow and cross-validation
Wf2 <- workflow() |> add_model(ModelSpec) |> add_recipe(Recipe2)

Results2 <- fit_resamples(
  Wf2, 
  resamples = CvFolds, 
  metrics = metric_set(rmse, rsq), 
  control = control_resamples(save_pred = TRUE)
  )

#Recipe 3 workflow and cross-validation
Wf3 <- workflow() |> add_model(ModelSpec) |> add_recipe(Recipe3)

Results3 <- fit_resamples(
  Wf3, 
  resamples = CvFolds, 
  metrics = metric_set(rmse, rsq), 
  control = control_resamples(save_pred = TRUE)
  )

#Compare the 3 models
bind_rows(
  Results1 = collect_metrics(Results1),
  Results2 = collect_metrics(Results2),
  Results3 = collect_metrics(Results3),
  .id = "Recipe"
)


#Final Fit
FinalFit <- last_fit(Wf2, DailyDataSplit)
collect_metrics(FinalFit)

#Extract coefficients
FinalModel <- extract_fit_parsnip(FinalFit)
tidy(FinalModel)

```
**The 10-fold cross-validation shows that Recipe 3 was the best model to choose as the final given it had the lowest RMSE (2768) and highest R-Squared (0.925). The recipe that contains both the interactions as well as the squared terms had the best predictive accuracy of the three recipes. When the interactions were included in the model, there wasa big increase in predictive accuracy (Recipe 1: RMSE = 3895 / RSq = 0.850 vs Recipe 2: RMSE = 0.922 / RSq = 0.922). Although it appears insignificant, the accuracy of the model further improved when the quadratic terms were introduced. [Recipe 3 > Recipe 2 > Recipe 1 (in terms of predictive accuracy)]**
